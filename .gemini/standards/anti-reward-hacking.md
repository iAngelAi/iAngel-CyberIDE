# Protocole Anti-Reward-Hacking

> **PrioritÃ©** : ABSOLUE  
> **Objectif** : EmpÃªcher les comportements de "triche" qui font passer les tests sans rÃ©soudre le vrai problÃ¨me

---

## ğŸš¨ DÃ©finition du Reward Hacking

Le **reward hacking** (ou "greenwashing de tests") se produit quand un agent IA :
- Modifie un test pour le faire passer au lieu de corriger le code
- Supprime des assertions qui Ã©chouent
- Ajoute des `skip` sans justification
- Contourne les validations au lieu de les corriger

**C'est la pire forme de dette technique** car elle masque les vrais problÃ¨mes.

---

## ğŸ›‘ Comportements STRICTEMENT INTERDITS

### 1. Modification de Tests pour les Faire Passer

```python
# âŒ INTERDIT - Modification du test
def test_calculate_sum():
    # Ancien: assert calculate_sum(2, 2) == 4
    assert calculate_sum(2, 2) == 5  # "CorrigÃ©" pour matcher le bug

# âœ… CORRECT - Correction du code
def calculate_sum(a: int, b: int) -> int:
    return a + b  # CorrigÃ© de "a + b + 1" Ã  "a + b"
```

### 2. Suppression d'Assertions

```python
# âŒ INTERDIT - Suppression d'assertion
def test_user_validation():
    user = create_user(invalid_data)
    # assert user.is_valid == False  # "SupprimÃ© car Ã©chouait"
    assert user is not None

# âœ… CORRECT - Correction de la logique
def test_user_validation():
    user = create_user(invalid_data)
    assert user.is_valid == False  # Assertion maintenue, code corrigÃ©
```

### 3. Skip InjustifiÃ©

```python
# âŒ INTERDIT
@pytest.mark.skip("Ã‰choue, Ã  corriger plus tard")
def test_critical_feature():
    ...

# âœ… ACCEPTABLE (avec justification technique)
@pytest.mark.skip(reason="DÃ©pend de l'API externe indisponible en CI - Issue #123")
def test_external_api_integration():
    ...
```

### 4. Catch-All pour Masquer les Erreurs

```python
# âŒ INTERDIT
def process_data(data):
    try:
        return risky_operation(data)
    except:  # Attrape tout, masque le vrai problÃ¨me
        return None

# âœ… CORRECT
def process_data(data: InputData) -> Result:
    try:
        return risky_operation(data)
    except ValidationError as e:
        logger.error(f"Validation failed: {e}")
        raise
    except NetworkError as e:
        logger.warning(f"Network issue, retrying: {e}")
        return retry_operation(data)
```

---

## âœ… Protocole Obligatoire Face Ã  un Test Ã‰chouÃ©

### Ã‰tape 1: STOP - Ne Pas Modifier

```
âš ï¸ TEST Ã‰CHOUÃ‰ DÃ‰TECTÃ‰
Fichier: tests/test_calculator.py::test_add
Erreur: AssertionError: assert 5 == 4

ğŸ›‘ ACTION: Ne PAS modifier le test immÃ©diatement
```

### Ã‰tape 2: Diagnostic

```
ğŸ“Š DIAGNOSTIC EN COURS

1. Lecture du test:
   - Assertion: calculate_sum(2, 2) == 4
   - Comportement attendu: 2 + 2 = 4 âœ“

2. Lecture du code source:
   - Fichier: src/calculator.py
   - Fonction: calculate_sum(a, b) -> a + b + 1
   - Comportement actuel: 2 + 2 + 1 = 5 âœ—

3. Verdict: Le CODE contient un bug (+ 1 en trop)
   Le TEST est correct.
```

### Ã‰tape 3: Demande d'Approbation

```
ğŸ” RAPPORT DE DIAGNOSTIC

ProblÃ¨me identifiÃ©: Bug dans calculate_sum() 
- Le code ajoute 1 de trop
- Le test est correct

Correction proposÃ©e:
- Modifier src/calculator.py ligne 15
- Retirer le "+ 1" parasite

âš ï¸ ATTENTE D'APPROBATION avant modification
```

### Ã‰tape 4: Correction (aprÃ¨s approbation)

```python
# src/calculator.py
def calculate_sum(a: int, b: int) -> int:
    return a + b  # CorrigÃ©: retirÃ© le "+ 1"
```

### Ã‰tape 5: VÃ©rification

```
âœ… CORRECTION APPLIQUÃ‰E

RÃ©sultat:
- tests/test_calculator.py::test_add PASSED
- Aucun test modifiÃ©
- Code source corrigÃ©

ğŸ§  Neural pathways restored. System optimal.
```

---

## ğŸ“‹ Checklist Anti-Reward-Hacking

Avant chaque modification suite Ã  un test Ã©chouÃ© :

- [ ] J'ai lu le test en entier
- [ ] J'ai compris ce que le test vÃ©rifie
- [ ] J'ai identifiÃ© si le problÃ¨me est dans le CODE ou le TEST
- [ ] Si le test est incorrect, j'ai une JUSTIFICATION technique
- [ ] J'ai demandÃ© l'approbation avant de modifier un test
- [ ] Je n'ai pas utilisÃ© `skip` sans issue tracker associÃ©e
- [ ] Je n'ai pas supprimÃ© d'assertions
- [ ] Je n'ai pas ajoutÃ© de try/except gÃ©nÃ©rique

---

## ğŸ”´ Signaux d'Alerte

Si tu te retrouves Ã  penser :
- "Ce test est trop strict, je vais l'assouplir" â†’ **STOP**
- "Je vais skipper Ã§a pour l'instant" â†’ **STOP**
- "L'assertion n'est pas vraiment nÃ©cessaire" â†’ **STOP**
- "Je vais juste attraper l'exception pour que Ã§a passe" â†’ **STOP**

Ces pensÃ©es sont des **signaux de reward hacking**. Reviens Ã  l'Ã©tape de diagnostic.

---

## ğŸ¯ Objectif Final

Un test qui Ã©choue est une **INFORMATION PRÃ‰CIEUSE**, pas un obstacle Ã  Ã©liminer.

Le but n'est pas d'avoir tous les tests verts. 
Le but est d'avoir du **code correct** vÃ©rifiÃ© par des tests fiables.
